# -*- coding: utf-8 -*-
"""LVADSUSR77_Bertina_lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YH2P1gchuKuD6_qhCnUHX3oyj8a_Hw-F
"""

#importing modules
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

#to supress warnings
import warnings as wr
wr.filterwarnings("ignore")

#reading data
data = pd.read_csv("/content/social_network.csv")
data.head()

data.columns

print("Missing values before using ffill: ")
missing_values = data.isnull().sum()
missing_values

"""No missing data"""

#no need
#data.fillna(method='ffill', inplace=True)

#Handling outliers
data.shape

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)
print(outliers)

data = data[~outliers]

data.shape

"""No outliers found"""

#Exploratory Data Analysis
print("Shape of the data:", data.shape)
print("Descriptive statistics:")
print(data.describe())

#Visualization for the features
for i in data.columns:
    plt.figure()
    sns.histplot(data[i], kde=True)
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.show()

#Define features and target
X = data[['login_activity', 'posting_activity', 'social_connections']]
y = data['suspicious_activity']

#Spliting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = IsolationForest(n_estimators=100, contamination=0.1, max_features=3, max_samples=10000, random_state=42)
model.fit(X_train)

y_pred = model.predict(X_train)

data["anomaly_score"] = model.decision_function(X)

anomalies = data.loc[data["anomaly_score"] < 0]

plt.scatter(data["social_connections"], data["anomaly_score"], label="Not Anomaly")
plt.scatter(anomalies["social_connections"], anomalies["anomaly_score"], color="r", label="Anomaly")
plt.xlabel("Social Connections")
plt.ylabel("Anomaly Score")
plt.legend()
plt.show()

# clf = IsolationForest(contamination=0.1)
# clf.fit(X_train)

y_pred = model.predict(X_test)

#Evaluating the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

